#!/bin/bash
set -eou pipefail

env

[[ "${DEBUG:-}" ]] && set -x

# check for existence of required env. vars.
: "$FASTCAT_S3URL_OUTPUT_PREFIX" # e.g. s3://hkgi-fastq-test2/outputs
: "$FASTCAT_S3URL_JOBS_PREFIX" # e.g. s3://hkgi-fastq-test2/dev/jobs


(( $# == 1 )) || {
    cat >&2 <<EOF
$0: <job_file_s3>
1) Download the job descriptions in <job_file_s3> from S3.
2) create one concatenated FASTQ.gz file as a subjob of an AWS Batch Job array
   and upload it to S3.
3) upload the MD5 to S3.

where
    <job_file_s3> is an s3:// URL to the job file

Environment variables *required*:
    FASTCAT_S3URL_OUTPUT_PREFIX (format: s3://<bucket>/<prefix>)
        S3 URL prefix for output, concatenated FASTQ.gz file
    FASTCAT_S3URL_JOBS_PREFIX (format: s3://<bucket>/<prefix>)
        S3 URL prefix for intermediate files of jobs
EOF

    exit 1
}


upload_chunk_size=$((128 * 1024 * 1024)) # 128MB

job_index="${FASTCAT_JOB_ARRAY_INDEX:-${AWS_BATCH_JOB_ARRAY_INDEX:?This script should be run in a AWS Batch *Array* job}}"
job_file_url="$1"


cd /tmp
aws s3 cp "$job_file_url" job.json


input_s3_url_prefix="$(jq -r '.input_s3_url_prefix' < job.json)"
job_id="$(jq -r '.job_id' < job.json)"
batch_id="$(jq -r '.batch_id' < job.json)"
[[ -n "$input_s3_url_prefix" ]] || {
    echo "cannot determine input S3 URL prefix from $job_file_url" >& 2
    exit 1
}
[[ -n "$job_id" ]] || {
    echo "cannot determine job id from $job_file_url" >& 2
    exit 1
}
[[ -n "$batch_id" ]] || {
    echo "cannot determine batch id from $job_file_url" >& 2
    exit 1
}
echo "job $job_id subjob $job_index"

output_s3_url_prefix="$FASTCAT_S3URL_OUTPUT_PREFIX/$batch_id"
job_s3_prefix="$FASTCAT_S3URL_JOBS_PREFIX/$job_id"


jq ".jobs[$job_index]" < job.json > subjob.json
sample="$(jq -r '.sample' < subjob.json)"
read_end="$(jq -r '.end' < subjob.json)"
[[ -n "$sample" && -n "$read_end" ]] || {
    echo "cannot determine lib ID or read end from $job_file_url"
    exit 1
}
echo "for $sample read $read_end"

mapfile -t read_counts < <(jq   '.counts | .[]' < subjob.json)
mapfile -t files  < <(jq -r '.files | .[]' < subjob.json)



wait_for_file() {
    local filename="$1"
    until [[ -e "$filename" ]]; do
        echo "waiting for $filename..." >& 2
        sleep 1;
    done
}

s3_concat_and_count_reads() {
    local total_reads=0

    for i in "${!files[@]}"; do
        local file="${files[i]}"
        local read_count_expected="${read_counts[i]}"
        local s3_url="$input_s3_url_prefix/$file"
        local line_count_file=./line_count.tmp

        rm -f "$line_count_file"
        aws s3 cp "$s3_url" - |\
            tee >(zcat | wc -l > "$line_count_file.tmp" &&
                      mv -f "$line_count_file"{.tmp,})
        wait_for_file "$line_count_file"

        local lines
        lines="$(cat "$line_count_file")"
        ((lines % 4 == 0)) || {
            echo "Line count ($lines) is not a multiple of 4... truncated file? <== $s3_url" >& 2
            exit 1
        }

        local read_count_found=$((lines / 4))
        ((total_reads += read_count_found))
        ((read_count_expected < 1)) && {
            echo "$read_count_found reads found <== $s3_url" >&2
            continue
        }

        ((read_count_found == read_count_expected)) || {
            echo "Read count mismatch (expected=$read_count_expected found=$read_count_found) <== $s3_url" >& 2
            exit 1
        }

        echo "Read count = $read_count_expected as expected <== $s3_url" >& 2
    done

    echo "$total_reads" > read_count.total
}

calc_md5() {
    local out_file="$1"
    local tmp_file="$out_file.tmp"

    rm -f "$out_file"
    md5sum > "$tmp_file"
    mv -f "$tmp_file" "$out_file"
}



fastq_base="${sample}_${read_end}"
out_url="$output_s3_url_prefix/$fastq_base.fastq.gz"
stats_out_url="$job_s3_prefix/$fastq_base.fastq.stats"


echo "Concatenating from $input_s3_url_prefix ..."
for i in "${files[@]}"; do
    echo "  $i"
done
echo "==>$out_url"
echo "   $stats_out_url"

s3_concat_and_count_reads |\
    tee >(calc_md5 md5) |\
    aws s3 cp --expected-size $((10000 * upload_chunk_size)) - "$out_url"

read_count="$(cat read_count.total)"

wait_for_file md5
md5="$(cut -d\  -f1 md5)"

echo "$md5"$'\t'"$read_count" |\
    aws s3 cp - "$stats_out_url"


echo "Done $fastq_base"

# end
