#!/bin/bash
set -euo pipefail
env

[[ "${DEBUG:-}" ]] && set -x
: "${NO_SUBMIT_JOBS:-}" # for debugging: do not really submit new jobs

# check for existence of required env. vars.
: "$FASTCAT_S3URL_OUTPUT_PREFIX" # e.g. s3://hkgi-fastq-test2/outputs
: "$FASTCAT_S3URL_JOBS_PREFIX" # e.g. s3://hkgi-fastq-test2/dev/jobs
: "$FASTCAT_JOB_QUEUE_ARN"
: "$FASTCAT_CONCAT_JOB_DEF_ARN"
: "$FASTCAT_METADATA_GENERATION_JOB_DEF_ARN"


(( $# == 1 )) || {
    cat >&2 <<EOF
$0: <fastq_list>
Parse input metadata files and create a job file containing
data for the jobs to be spawned

where
    <fastq_list> is an s3:// URL to the input fastq_list.csv;
                 the accompanying Demultiplex_Stats.csv file is
                 assumed to be found under the same directory;
                 All input fastq.gz files should also be found there.

Environment variables *required*:
    FASTCAT_S3URL_OUTPUT_PREFIX (format: s3://<bucket>/<prefix>)
        S3 URL prefix for output (concatenated FASTQ and metadata)
    FASTCAT_S3URL_JOBS_PREFIX (format: s3://<bucket>/<prefix>)
        S3 URL prefix for intermediate files of jobs
    FASTCAT_JOB_QUEUE_ARN
        ARN of Job queue defined in AWS Batch
    FASTCAT_CONCAT_JOB_DEF_ARN
        ARN of Job definition for the concat job (array)
    FASTCAT_METADATA_GENERATION_JOB_DEF_ARN
        ARN of Job definition for the metadata generation job
EOF

    exit 1
}

fqlist_url="$1"

job_rand="$(echo "${AWS_BATCH_JOB_ID:-$(uuidgen)}" | cut -d- -f1)"
job_ts="$(TZ=UTC date +%Y%m%d-%H%M%S)"
job_id="$job_ts-$job_rand"
in_dir_url="${fqlist_url%/*}"
demux_url="$in_dir_url/Demultiplex_Stats.csv"
batch_id="${in_dir_url##*/}"

[[ "$batch_id" ]] || {
    echo "Cannot derive batch id from: $fqlist_url" >& 2
    exit 1
}

job_s3="$FASTCAT_S3URL_JOBS_PREFIX/$job_id"
job_file_s3="$job_s3/job.json"


mkdir -p /tmp/job/data
cd /tmp/job


aws s3 cp "$fqlist_url" fqlist.csv
aws s3 cp "$demux_url"  demux.csv

node --enable-source-maps /opt/nodejs/dist/bcl2fq-metadata-parser.js \
     demux.csv fqlist.csv \
     "$in_dir_url" "$job_id" "$batch_id" \
     job.json

aws s3 cp job.json "$job_file_s3"

nr_subjobs="$(jq '.jobs | length' < job.json)"


${NO_SUBMIT_JOBS:+echo Not submitting: } \
aws batch submit-job \
    --job-name "fastcat-$job_id" \
    --job-queue "$FASTCAT_JOB_QUEUE_ARN" \
    --job-definition "$FASTCAT_CONCAT_JOB_DEF_ARN" \
    --parameters "job_file_s3_url=$job_file_s3" \
    --array-properties "size=$nr_subjobs" |\
    tee concat-job.json
if [[ "${NO_SUBMIT_JOBS:-}" ]]; then
    concat_job_id="(unsubmitted)"
else
    concat_job_id="$(jq -r ".jobId" < concat-job.json)"
fi
#FIXME:    --tags ???


${NO_SUBMIT_JOBS:+echo Not submitting: } \
aws batch submit-job \
    --job-name "fastcat-consolidate-$job_id" \
    --job-queue "$FASTCAT_JOB_QUEUE_ARN" \
    --job-definition "$FASTCAT_METADATA_GENERATION_JOB_DEF_ARN" \
    --parameters "job_file_s3_url=$job_file_s3" \
    --depends-on "jobId=$concat_job_id,type=SEQUENTIAL"
#FIXME:    --tags ???

#end
